{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bf4c22-6ca0-4ead-bd06-37dc67d48033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from attrdict import AttrDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import metrics\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from tez import Tez, TezConfig\n",
    "from tez.callbacks import EarlyStopping\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf29156-f538-4977-99af-a18e894191f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    # Model Configs\n",
    "    \"model\": \"microsoft/deberta-v3-large\",\n",
    "    \"max_len\": 512,\n",
    "    \n",
    "    # Train Configs\n",
    "    \"fold_num\": 4,\n",
    "    \"val_fold\": 0,\n",
    "    \"lr\": 2e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"valid_batch_size\": 64,\n",
    "    \"epochs\": 4, # Set to 1 because it is a demo\n",
    "    \"accumulation_steps\": 1,\n",
    "    \"val_steps\": 430,\n",
    "    \"max_norm\": -1,\n",
    "    \n",
    "    # GPU Optimize Settings\n",
    "    \"gpu_optimize_config\": {\n",
    "        \"fp16\": True,\n",
    "        \"freezing\": True,\n",
    "        \"optim8bit\": True,\n",
    "        \"gradient_checkpoint\": True\n",
    "    },\n",
    "    \n",
    "    # Path\n",
    "    \"input\": \"../../data\",\n",
    "    \"output\": \"./outputs\"\n",
    "}\n",
    "cfg = AttrDict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda1c843-538f-4160-87f4-e7329fee1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4a352a-b509-445e-83ba-9f199bea5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", type=str, required=True)\n",
    "    parser.add_argument(\"--fold\", type=int, required=False, default=0)\n",
    "    #parser.add_argument(\"--model\", type=str, required=False, default=\"microsoft/deberta-base\")\n",
    "    #parser.add_argument(\"--lr\", type=float, required=False, default=3e-5)\n",
    "    parser.add_argument(\"--output\", type=str, default=\".\", required=False)\n",
    "    #parser.add_argument(\"--input\", type=str, default=\"../input\", required=False)\n",
    "    #parser.add_argument(\"--max_len\", type=int, default=1024, required=False)\n",
    "    #parser.add_argument(\"--batch_size\", type=int, default=2, required=False)\n",
    "    #parser.add_argument(\"--valid_batch_size\", type=int, default=16, required=False)\n",
    "    #parser.add_argument(\"--epochs\", type=int, default=5, required=False)\n",
    "    #parser.add_argument(\"--accumulation_steps\", type=int, default=1, required=False)\n",
    "    #parser.add_argument(\"--predict\", action=\"store_true\", required=False)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def _prepare_training_data_helper(cfg, tokenizer, df, is_train):\n",
    "    training_samples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        idx = row[\"essay_id\"]\n",
    "        discourse_text = row[\"discourse_text\"]\n",
    "        discourse_type = row[\"discourse_type\"]\n",
    "\n",
    "        if is_train:\n",
    "            filename = os.path.join(cfg.input, \"train\", idx + \".txt\")\n",
    "        else:\n",
    "            filename = os.path.join(cfg.input, \"test\", idx + \".txt\")\n",
    "\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            discourse_type + \"[SEP]\" + discourse_text,\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "\n",
    "        sample = {\n",
    "            \"discourse_id\": row[\"discourse_id\"],\n",
    "            \"input_ids\": input_ids,\n",
    "            # \"discourse_text\": discourse_text,\n",
    "            # \"essay_text\": text,\n",
    "            # \"mask\": encoded_text[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "        if \"token_type_ids\" in encoded_text:\n",
    "            sample[\"token_type_ids\"] = encoded_text[\"token_type_ids\"]\n",
    "\n",
    "        label = row[\"discourse_effectiveness\"]\n",
    "\n",
    "        sample[\"label\"] = LABEL_MAPPING[label]\n",
    "\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "def prepare_training_data(df, tokenizer, cfg, num_jobs, is_train):\n",
    "    training_samples = []\n",
    "\n",
    "    df_splits = np.array_split(df, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)(cfg, tokenizer, df, is_train) for df in df_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "\n",
    "    return training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9329ef95-680c-4f73-b52c-ae77a0fcc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackDataset:\n",
    "    def __init__(self, samples, cfg, tokenizer):\n",
    "        self.samples = samples\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = self.samples[idx][\"input_ids\"]\n",
    "        label = self.samples[idx][\"label\"]\n",
    "\n",
    "        input_ids = [self.tokenizer.cls_token_id] + ids\n",
    "\n",
    "        if len(input_ids) > self.cfg.max_len - 1:\n",
    "            input_ids = input_ids[: self.cfg.max_len - 1]\n",
    "\n",
    "        input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        mask = [1] * len(input_ids)\n",
    "\n",
    "        return {\n",
    "            \"ids\": input_ids,\n",
    "            \"mask\": mask,\n",
    "            # \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": label,\n",
    "        }\n",
    "\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, cfg):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n",
    "        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n",
    "        output[\"targets\"] = [sample[\"targets\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
    "        else:\n",
    "            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n",
    "        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n",
    "        output[\"targets\"] = torch.tensor(output[\"targets\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb65c27-4ec2-4f91-b02d-15940ce12be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "\n",
    "def set_embedding_parameters_bits(embeddings_path, optim_bits=32):\n",
    "    \"\"\"\n",
    "    https://github.com/huggingface/transformers/issues/14819#issuecomment-1003427930\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_types = (\"word\", \"position\", \"token_type\")\n",
    "    for embedding_type in embedding_types:\n",
    "        attr_name = f\"{embedding_type}_embeddings\"\n",
    "        \n",
    "        if hasattr(embeddings_path, attr_name): \n",
    "            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n",
    "                getattr(embeddings_path, attr_name), 'weight', {'optim_bits': optim_bits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b193a68d-a059-4607-96c1-bddabb5e66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0492d59a-7169-48da-b051-0cdd12265858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackModel(nn.Module):\n",
    "    def __init__(self, model_name, num_train_steps, learning_rate, num_labels, steps_per_epoch, gpu_optimize_config):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_name = model_name\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.num_labels = num_labels\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.gpu_optimize_config = gpu_optimize_config\n",
    "\n",
    "        hidden_dropout_prob: float = 0.1\n",
    "        layer_norm_eps: float = 1e-7\n",
    "\n",
    "        model_config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        model_config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "                \"num_labels\": self.num_labels,\n",
    "            }\n",
    "        )\n",
    "        self.transformer = AutoModel.from_pretrained(model_name, config=model_config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.dropout1 = nn.Dropout(0.08)\n",
    "        self.dropout2 = nn.Dropout(0.09)\n",
    "        self.dropout3 = nn.Dropout(0.10)\n",
    "        self.dropout4 = nn.Dropout(0.11)\n",
    "        self.dropout5 = nn.Dropout(0.12)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.output = nn.Linear(model_config.hidden_size, self.num_labels)\n",
    "\n",
    "        # Freeze\n",
    "        if self.gpu_optimize_config.freezing:\n",
    "            freeze(self.transformer.embeddings)\n",
    "            freeze(self.transformer.encoder.layer[:2])\n",
    "\n",
    "        # Gradient Checkpointing\n",
    "        if self.gpu_optimize_config.gradient_checkpoint:\n",
    "            self.transformer.gradient_checkpointing_enable()  \n",
    "\n",
    "\n",
    "    def optimizer_scheduler(self):\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "                \"weight_decay\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        opt = AdamW(optimizer_parameters, lr=self.learning_rate, weight_decay=0.01)\n",
    "        if self.gpu_optimize_config.gradient_checkpoint:\n",
    "            opt = bnb.optim.AdamW(optimizer_parameters, lr=self.learning_rate, weight_decay=0.01, optim_bits=8)\n",
    "            #set_embedding_parameters_bits(embeddings_path=self.transformer.embeddings)\n",
    "        sch = get_linear_schedule_with_warmup(\n",
    "            opt,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=self.num_train_steps,\n",
    "            last_epoch=-1,\n",
    "        )\n",
    "        return opt, sch\n",
    "\n",
    "    def loss(self, outputs, targets):\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        device = targets.get_device()\n",
    "        mll = metrics.log_loss(\n",
    "            targets.cpu().detach().numpy(),\n",
    "            outputs.cpu().detach().numpy(),\n",
    "            labels=[0, 1, 2],\n",
    "        )\n",
    "        return {\"mll\": torch.tensor(mll, device=device)}\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None):\n",
    "\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.transformer(ids, mask, token_type_ids)\n",
    "        else:\n",
    "            transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out.last_hidden_state\n",
    "        sequence_output = self.pooler(sequence_output, mask)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        logits1 = self.output(self.dropout1(sequence_output))\n",
    "        logits2 = self.output(self.dropout2(sequence_output))\n",
    "        logits3 = self.output(self.dropout3(sequence_output))\n",
    "        logits4 = self.output(self.dropout4(sequence_output))\n",
    "        logits5 = self.output(self.dropout5(sequence_output))\n",
    "\n",
    "        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        loss = 0\n",
    "\n",
    "        if targets is not None:\n",
    "            loss1 = self.loss(logits1, targets)\n",
    "            loss2 = self.loss(logits2, targets)\n",
    "            loss3 = self.loss(logits3, targets)\n",
    "            loss4 = self.loss(logits4, targets)\n",
    "            loss5 = self.loss(logits5, targets)\n",
    "            loss = (loss1 + loss2 + loss3 + loss4 + loss5) / 5\n",
    "            metric = self.monitor_metrics(logits, targets)\n",
    "            return logits, loss, metric\n",
    "\n",
    "        return logits, loss, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caea1cee-6d09-47ff-9b35-276aeff9432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "NUM_JOBS = 12\n",
    "seed_everything(42)\n",
    "os.makedirs(cfg.output, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca91be75-31cf-4035-bd92-524ae9dc159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfold  discourse_effectiveness\n",
       "0      Adequate                   5242\n",
       "       Effective                  2331\n",
       "       Ineffective                1615\n",
       "1      Adequate                   5236\n",
       "       Effective                  2331\n",
       "       Ineffective                1615\n",
       "2      Adequate                   5246\n",
       "       Effective                  2334\n",
       "       Ineffective                1618\n",
       "3      Adequate                   5253\n",
       "       Effective                  2330\n",
       "       Ineffective                1614\n",
       "Name: discourse_effectiveness, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create fold\n",
    "df = pd.read_csv(os.path.join(cfg.input, \"train.csv\"))\n",
    "gkf = StratifiedGroupKFold(n_splits=cfg.fold_num)\n",
    "for fold, ( _, val_) in enumerate(gkf.split(X=df, y=df['discourse_effectiveness'], groups=df.essay_id)):\n",
    "    df.loc[val_ , \"kfold\"] = int(fold)\n",
    "\n",
    "df[\"kfold\"] = df[\"kfold\"].astype(int)\n",
    "df.groupby('kfold')['discourse_effectiveness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfd23d4f-4e8b-4fdc-8479-7f87a24f6ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 74%|███████████████████████████████████████████████▏                | 1693/2298 [00:03<00:01, 447.49it/s]\n",
      " 64%|█████████████████████████████████████████                       | 1474/2298 [00:03<00:01, 442.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:04<00:00, 478.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:04<00:00, 514.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:04<00:00, 484.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:05<00:00, 430.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:04<00:00, 477.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:04<00:00, 499.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:04<00:00, 514.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:04<00:00, 476.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:04<00:00, 463.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 2298/2298 [00:05<00:00, 416.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 766/766 [00:01<00:00, 662.20it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 766/766 [00:01<00:00, 661.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 766/766 [00:01<00:00, 644.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 766/766 [00:01<00:00, 570.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 766/766 [00:01<00:00, 672.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 766/766 [00:01<00:00, 583.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 766/766 [00:01<00:00, 670.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 766/766 [00:01<00:00, 692.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 765/765 [00:01<00:00, 665.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 765/765 [00:00<00:00, 769.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 765/765 [00:01<00:00, 734.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 765/765 [00:01<00:00, 547.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# DataSet Preparation\n",
    "train_df = df[df[\"kfold\"] != cfg.val_fold].reset_index(drop=True)\n",
    "valid_df = df[df[\"kfold\"] == cfg.val_fold].reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model, use_fast=True)\n",
    "training_samples = prepare_training_data(train_df, tokenizer, cfg, num_jobs=NUM_JOBS, is_train=True)\n",
    "valid_samples = prepare_training_data(valid_df, tokenizer, cfg, num_jobs=NUM_JOBS, is_train=True)\n",
    "\n",
    "training_samples = list(sorted(training_samples, key=lambda d: len(d[\"input_ids\"])))\n",
    "valid_samples = list(sorted(valid_samples, key=lambda d: len(d[\"input_ids\"])))\n",
    "\n",
    "train_dataset = FeedbackDataset(training_samples, cfg, tokenizer)\n",
    "valid_dataset = FeedbackDataset(valid_samples, cfg, tokenizer)\n",
    "\n",
    "num_train_steps = int(len(train_dataset) / cfg.batch_size / cfg.accumulation_steps * cfg.epochs)\n",
    "\n",
    "collate_fn = Collate(tokenizer, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b9001ec-da88-4caa-94ca-743e3ffd606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Model Preparation\n",
    "model = FeedbackModel(\n",
    "    model_name=cfg.model,\n",
    "    num_train_steps=num_train_steps,\n",
    "    learning_rate=cfg.lr,\n",
    "    num_labels=3,\n",
    "    steps_per_epoch=len(train_dataset) / cfg.batch_size,\n",
    "    gpu_optimize_config=cfg.gpu_optimize_config,\n",
    ")\n",
    "model = Tez(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2232c6e-71c4-4d58-ba05-017247cfec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2d019651e44c1683bb03ca8903655c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3448 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113a30cc93644c3683fda621a2073c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m      3\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39moutput, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_f\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mval_fold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m train_config \u001b[38;5;241m=\u001b[39m TezConfig(\n\u001b[1;32m     12\u001b[0m     training_batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     13\u001b[0m     validation_batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mvalid_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     val_steps\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mval_steps,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_collate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tez/model/tez.py:426\u001b[0m, in \u001b[0;36mTez.fit\u001b[0;34m(self, train_dataset, valid_dataset, config, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state \u001b[38;5;241m=\u001b[39m enums\u001b[38;5;241m.\u001b[39mTrainingState\u001b[38;5;241m.\u001b[39mEPOCH_START\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loader \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mval_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tez/model/tez.py:368\u001b[0m, in \u001b[0;36mTez.train\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch_index \u001b[38;5;241m=\u001b[39m batch_index\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state \u001b[38;5;241m=\u001b[39m enums\u001b[38;5;241m.\u001b[39mTrainingState\u001b[38;5;241m.\u001b[39mTRAIN_STEP_START\n\u001b[0;32m--> 368\u001b[0m loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m losses, monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_loss_metrics(losses, loss, metrics)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state \u001b[38;5;241m=\u001b[39m enums\u001b[38;5;241m.\u001b[39mTrainingState\u001b[38;5;241m.\u001b[39mTRAIN_STEP_END\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tez/model/tez.py:287\u001b[0m, in \u001b[0;36mTez.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad()\n\u001b[0;32m--> 287\u001b[0m     _, loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward(loss)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_grad_norm()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tez/model/tez.py:251\u001b[0m, in \u001b[0;36mTez.model_fn\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m--> 251\u001b[0m     output, loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accel\u001b[38;5;241m.\u001b[39mgather(metrics)\n\u001b[1;32m    253\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/utils/operations.py:487\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autocast_mode.py:198\u001b[0m, in \u001b[0;36mautocast.__call__.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 198\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mFeedbackModel.forward\u001b[0;34m(self, ids, mask, token_type_ids, targets)\u001b[0m\n\u001b[1;32m     87\u001b[0m     transformer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(ids, mask, token_type_ids)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     transformer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m transformer_out\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     91\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output, mask)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1050\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1042\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1043\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1044\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1048\u001b[0m )\n\u001b[0;32m-> 1050\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:490\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m module(\u001b[38;5;241m*\u001b[39minputs, output_attentions)\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_forward\n\u001b[0;32m--> 490\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_custom_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_module\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    500\u001b[0m         next_kv,\n\u001b[1;32m    501\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    506\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:211\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword arguments: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m kwargs))\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:90\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     87\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:486\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_forward\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:343\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    336\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m ):\n\u001b[0;32m--> 343\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    352\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:274\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    267\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m ):\n\u001b[0;32m--> 274\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    283\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:699\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[1;32m    698\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 699\u001b[0m     rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m rel_att\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:743\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelative position ids must be of dim 2 or 3 or 4. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_pos\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    742\u001b[0m att_span \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_ebd_size\n\u001b[0;32m--> 743\u001b[0m relative_pos \u001b[38;5;241m=\u001b[39m \u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m rel_embeddings \u001b[38;5;241m=\u001b[39m rel_embeddings[\u001b[38;5;241m0\u001b[39m : att_span \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_att_key:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "es = EarlyStopping(\n",
    "    monitor=\"valid_loss\",\n",
    "    model_path=os.path.join(cfg.output, f\"model_f{cfg.val_fold}.bin\"),\n",
    "    patience=5,\n",
    "    mode=\"min\",\n",
    "    delta=0.001,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "train_config = TezConfig(\n",
    "    training_batch_size=cfg.batch_size,\n",
    "    validation_batch_size=cfg.valid_batch_size,\n",
    "    gradient_accumulation_steps=cfg.accumulation_steps,\n",
    "    epochs=cfg.epochs,\n",
    "    clip_grad_norm=cfg.max_norm,\n",
    "    fp16=cfg.gpu_optimize_config.fp16,\n",
    "    step_scheduler_after=\"batch\",\n",
    "    val_strategy=\"batch\",\n",
    "    val_steps=cfg.val_steps,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    train_collate_fn=collate_fn,\n",
    "    valid_collate_fn=collate_fn,\n",
    "    callbacks=[es],\n",
    "    config=train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec93b14-335e-4714-8a5d-50b960b97292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
